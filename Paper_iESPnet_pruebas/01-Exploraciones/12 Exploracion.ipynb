{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibilidad de experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import torchaudio.transforms    as T\n",
    "import torch.optim              as optim\n",
    "import pandas                   as pd\n",
    "import numpy                    as np\n",
    "\n",
    "from torchvision       import transforms\n",
    "from torch.utils.data  import DataLoader\n",
    "from torch             import nn\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..','05-Train-Test')))\n",
    "from utilit_train_test import make_weights_for_balanced_classes\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..','..','iESPnet_SRC_main','utilities')))\n",
    "from Generator         import SeizureDatasetLabelTimev2, permute_spec, smoothing_label\n",
    "from Model             import iESPnet\n",
    "from TrainEval         import train_model_v2, test_model_v2, get_performance_indices\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('../../..','03 Dynamic-Spatial-Filtering')))\n",
    "from models            import DynamicSpatialFilter\n",
    "\n",
    "# set the seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direccion donde se encuentran los espectrogramas \n",
    "SPE_DIR        = '/media/martin/Disco2/Rns_Data/PITT_PI_EEG/'\n",
    "meta_data_file = '/media/martin/Disco2/Rns_Data/PITT_PI_EEG/METADATA/allfiles_metadata.csv'\n",
    "\n",
    "df_meta        = pd.read_csv(meta_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables iESPnet\n",
    "FREQ_MASK_PARAM    = 10\n",
    "TIME_MASK_PARAN    = 20\n",
    "N_CLASSES          = 1\n",
    "learning_rate      = 1e-3\n",
    "batch_size         = 64    #128\n",
    "epochs             = 20\n",
    "num_workers        = 4\n",
    "\n",
    "save_path          = 'models_DSF_iESPnet_prueba/'\n",
    "patients           = df_meta['rns_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables DSF\n",
    "denoising          = 'autoreject'   # 'autoreject' 'data_augm' \n",
    "model              = 'stager_net'\n",
    "dsf_type           = 'dsfd'         # 'dsfd' 'dsfm_st'\n",
    "mlp_input          = 'log_diag_cov'\n",
    "dsf_soft_thresh    = False\n",
    "dsf_n_out_channels = None\n",
    "n_channels         = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiperparametros iESPnet\n",
    "hparams = {\n",
    "           \"n_cnn_layers\" : 3,\n",
    "           \"n_rnn_layers\" : 3,\n",
    "           \"rnn_dim\"      : [150, 100, 50],\n",
    "           \"n_class\"      : N_CLASSES,\n",
    "           \"out_ch\"       : [8,8,16],\n",
    "           \"dropout\"      : 0.3,\n",
    "           \"learning_rate\": learning_rate,\n",
    "           \"batch_size\"   : batch_size,\n",
    "           \"num_workers\"  : num_workers,\n",
    "           \"epochs\"       : epochs\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train y test de df_meta\n",
    "test_id  = ['PIT-RNS1090', 'PIT-RNS8973', 'PIT-RNS1438', 'PIT-RNS8326', 'PIT-RNS3016']\n",
    "vali_id  = ['PIT-RNS1603', 'PIT-RNS1556', 'PIT-RNS1534', 'PIT-RNS6989', 'PIT-RNS2543', 'PIT-RNS7168', 'PIT-RNS6762']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_meta.copy() # hace falta resetear el indice de train_df?\n",
    "test_df  = pd.DataFrame()\n",
    "vali_df  = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range (len(test_id)):\n",
    "    test_df = pd.concat([test_df, df_meta[df_meta['rns_id'] == test_id[s]]])\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "    train_df.drop(train_df[train_df['rns_id'] == test_id[s]].index, inplace = True)\n",
    "\n",
    "for s in range(len(vali_id)):\n",
    "    vali_df=pd.concat([vali_df, df_meta[df_meta['rns_id'] == vali_id[s]]])\n",
    "    vali_df.reset_index(drop=True, inplace=True)\n",
    "    train_df.drop(train_df[train_df['rns_id'] == vali_id[s]].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimentos que se van a realizar\n",
    "experiments_1 = ['exp1','exp2','exp3']\n",
    "experiments_2 = ['.1','.2','.3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = DynamicSpatialFilter(\n",
    "                              n_channels, \n",
    "                              mlp_input            = mlp_input, \n",
    "                              n_out_channels       = dsf_n_out_channels, \n",
    "                              apply_soft_thresh    = dsf_soft_thresh\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = iESPnet(\n",
    "                 hparams['n_cnn_layers'],\n",
    "                 hparams['n_rnn_layers'],\n",
    "                 hparams['rnn_dim'],\n",
    "                 hparams['n_class'],\n",
    "                 hparams['out_ch'],\n",
    "                 hparams['dropout'],\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_runs        = save_path + experiments_1[s] + '/' + str(experiments_1[s]) + str(experiments_2[j]) + '/runs/'\n",
    "save_models      = save_path + experiments_1[s] + '/' + str(experiments_1[s]) + str(experiments_2[j]) + '/models/'\n",
    "save_predictions = save_path + experiments_1[s] + '/' + str(experiments_1[s]) + str(experiments_2[j]) + '/results/'\n",
    "save_figs        = save_path + experiments_1[s] + '/' + str(experiments_1[s]) + str(experiments_2[j]) + '/figs/'\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "if not os.path.exists(save_runs):\n",
    "    os.makedirs(save_runs)\n",
    "\n",
    "if not os.path.exists(save_models):\n",
    "    os.makedirs(save_models)\n",
    "\n",
    "if not os.path.exists(save_predictions):\n",
    "    os.makedirs(save_predictions)\n",
    "\n",
    "if not os.path.exists(save_figs):\n",
    "    os.makedirs(save_figs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for: exp1.1\n"
     ]
    }
   ],
   "source": [
    "print('Running training for: ' + experiments_1[s] +  experiments_2[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders creados\n",
    "train_data = SeizureDatasetLabelTimev2(\n",
    "                                       file             = train_df,\n",
    "                                       root_dir         = SPE_DIR,\n",
    "                                       transform        = None, \n",
    "                                       target_transform = smoothing_label(),\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing data should be balanced, just be \"as it is\"\n",
    "test_data  = SeizureDatasetLabelTimev2(\n",
    "                                       file             = test_df,\n",
    "                                       root_dir         = SPE_DIR,\n",
    "                                       transform        = None,\n",
    "                                       target_transform = smoothing_label()  \n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data should be balanced, just be \"as it is\"\n",
    "vali_data  = SeizureDatasetLabelTimev2(\n",
    "                                       file             = vali_df,\n",
    "                                       root_dir         = SPE_DIR,\n",
    "                                       transform        = None,\n",
    "                                       target_transform = smoothing_label()  \n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation \n",
    "transform_train = transforms.Compose([\n",
    "                                      T.FrequencyMasking(FREQ_MASK_PARAM),\n",
    "                                      T.TimeMasking(TIME_MASK_PARAN), \n",
    "                                      permute_spec()                                                                     \n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = make_weights_for_balanced_classes(train_df, [0,1], n_concat=1)\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfile = save_models + 'model_' + str(experiments_1[s] + experiments_2[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\navg_train_losses, train_accs, avg_valid_losses, valid_accs = train_model_v2(\\n                                                                            model1, \\n                                                                            model2, \\n                                                                            hparams, \\n                                                                            epochs, \\n                                                                            train_data, \\n                                                                            vali_data, \\n                                                                            transform_train, \\n                                                                            sampler, \\n                                                                            outputfile,\\n                                                                            experiments_1[s],\\n                                                                            experiments_2[j]\\n                                                                           )\\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "avg_train_losses, train_accs, avg_valid_losses, valid_accs = train_model_v2(\n",
    "                                                                            model1, \n",
    "                                                                            model2, \n",
    "                                                                            hparams, \n",
    "                                                                            epochs, \n",
    "                                                                            train_data, \n",
    "                                                                            vali_data, \n",
    "                                                                            transform_train, \n",
    "                                                                            sampler, \n",
    "                                                                            outputfile,\n",
    "                                                                            experiments_1[s],\n",
    "                                                                            experiments_2[j]\n",
    "                                                                           )\n",
    "\n",
    "'''                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train_losses = []\n",
    "train_accs       = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_valid_losses = [] \n",
    "valid_accs       = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following pytorch suggestion to speed up training\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': hparams[\"num_workers\"], 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = hparams[\"batch_size\"], sampler = sampler, **kwargs)\n",
    "valid_loader = DataLoader(vali_data, batch_size = hparams[\"batch_size\"], shuffle = False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicSpatialFilter(\n",
       "  (feat_extractor): SpatialFeatureExtractor()\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=4, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=4, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#move model1 to device\n",
    "model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iESPnet(\n",
       "  (freqcnn): Sequential(\n",
       "    (0): Conv2d(4, 8, kernel_size=(120, 1), stride=(1, 1), padding=(119, 0), dilation=(2, 1), bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): InstanceNorm2d(8, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (timecnn): Sequential(\n",
       "    (0): Conv2d(4, 8, kernel_size=(1, 181), stride=(1, 1), padding=(0, 180), dilation=(1, 2), bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): InstanceNorm2d(8, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (cnn_ori): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)\n",
       "  (cnn): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)\n",
       "  (rescnn_layers): Sequential(\n",
       "    (0): ResidualCNNbatch(\n",
       "      (cnn1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (cnn2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "      (norm1): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (norm2): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ResidualCNNbatch(\n",
       "      (cnn1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (cnn2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "      (norm1): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (norm2): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): ResidualCNNbatch(\n",
       "      (cnn1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (cnn2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "      (norm1): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (norm2): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (rescnn_layers_ori): Sequential(\n",
       "    (0): ResidualCNNbatch(\n",
       "      (cnn1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (cnn2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "      (norm1): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (norm2): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ResidualCNNbatch(\n",
       "      (cnn1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (cnn2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "      (norm1): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (norm2): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): ResidualCNNbatch(\n",
       "      (cnn1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (cnn2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "      (norm1): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (norm2): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(40, 181))\n",
       "  (rnn_layers): Sequential(\n",
       "    (0): BidirectionalGRU(\n",
       "      (BiGRU): GRU(1280, 150, batch_first=True, bidirectional=True)\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (1): BidirectionalGRU(\n",
       "      (BiGRU): GRU(300, 100, bidirectional=True)\n",
       "      (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (2): BidirectionalGRU(\n",
       "      (BiGRU): GRU(200, 50, bidirectional=True)\n",
       "      (layer_norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#move model2 to device\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Model Parameters 120\n",
      "Num Model Parameters 1654837\n"
     ]
    }
   ],
   "source": [
    "print('Num Model Parameters', sum([param1.nelement() for param1 in model1.parameters()]))\n",
    "print('Num Model Parameters', sum([param2.nelement() for param2 in model2.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = optim.AdamW(model1.parameters(), hparams['learning_rate'], weight_decay=1e-4)\n",
    "optimizer2 = optim.AdamW(model2.parameters(), hparams['learning_rate'], weight_decay=1e-4)\n",
    "\n",
    "scheduler1 = optim.lr_scheduler.OneCycleLR(\n",
    "                                           optimizer1, \n",
    "                                           max_lr          = hparams['learning_rate'], \n",
    "                                           steps_per_epoch = int(len(train_loader)),\n",
    "                                           epochs          = hparams['epochs'],\n",
    "                                           anneal_strategy = 'linear'\n",
    "                                          )\n",
    "\n",
    "scheduler2 = optim.lr_scheduler.OneCycleLR(\n",
    "                                           optimizer2, \n",
    "                                           max_lr          = hparams['learning_rate'], \n",
    "                                           steps_per_epoch = int(len(train_loader)*2),\n",
    "                                           epochs          = hparams['epochs'],\n",
    "                                           anneal_strategy = 'linear'\n",
    "                                          )\n",
    "      \n",
    "criterion = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "train_losses, train_aucpr = training_DSF_iESPnet(\n",
    "                                                 model1, \n",
    "                                                 model2, \n",
    "                                                 device, \n",
    "                                                 train_loader, \n",
    "                                                 transform_train, \n",
    "                                                 criterion, \n",
    "                                                 optimizer1, \n",
    "                                                 optimizer2, \n",
    "                                                 scheduler1, \n",
    "                                                 scheduler2, \n",
    "                                                 epoch,\n",
    "                                                 experiment_1,\n",
    "                                                 experiment_2\n",
    "                                                )\n",
    "\n",
    "'''                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1 = experiments_1[s]\n",
    "experiment_2 = experiments_2[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spectrogram\n",
    "ECOG_SAMPLE_RATE = 250\n",
    "ECOG_CHANNELS    = 4\n",
    "TT               = 1000 # window length\n",
    "SPEC_WIN_LEN     = int(ECOG_SAMPLE_RATE * TT / 1000 ) # win size\n",
    "overlap          = 500 \n",
    "SPEC_HOP_LEN     = int(ECOG_SAMPLE_RATE * (TT - overlap) / 1000) # Length of hop between windows.\n",
    "SPEC_NFFT        = 500  # to see changes in 0.5 reso\n",
    "if   experiment_2 == '.1':  \n",
    "    top_db       = 40.0\n",
    "elif experiment_2 == '.2':\n",
    "    top_db       = 60.0\n",
    "elif experiment_2 == '.3':\n",
    "    top_db       = 80.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_thalamus)",
   "language": "python",
   "name": "env_thalamus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
