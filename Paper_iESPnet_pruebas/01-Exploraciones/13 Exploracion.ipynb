{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iESPnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import gc\n",
    "\n",
    "import torchaudio.transforms    as T\n",
    "import torch.optim              as optim\n",
    "import pandas                   as pd\n",
    "import numpy                    as np\n",
    "\n",
    "from torchvision       import transforms\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..','05-Train-Test')))\n",
    "from utilit_train_test import make_weights_for_balanced_classes\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..','..','iESPnet_SRC_main','utilities')))\n",
    "from Generator         import SeizureDatasetLabelTime, permute_spec, smoothing_label\n",
    "from Model             import iESPnet\n",
    "from TrainEval         import train_model_iespnet, test_model_iespnet, get_performance_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direccion donde se encuentran los espectrogramas (path: martin)\n",
    "SPE_DIR        = '/media/martin/Disco2/Rns_Data/PITT_PI_SPEC/'\n",
    "meta_data_file = '/media/martin/Disco2/Rns_Data/PITT_PI_SPEC/METADATA/allfiles_metadata.csv'\n",
    "\n",
    "df_meta = pd.read_csv(meta_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables iESPnet\n",
    "FREQ_MASK_PARAM = 10\n",
    "TIME_MASK_PARAN = 20\n",
    "N_CLASSES       = 1\n",
    "learning_rate   = 1e-3\n",
    "batch_size      = 128\n",
    "epochs          = 20\n",
    "num_workers     = 4\n",
    "save_path       = 'iespnet_global/'\n",
    "patients        = df_meta['rns_id'].unique().tolist()\n",
    "\n",
    "# hiperparametros iESPnet\n",
    "hparams = {\n",
    "        \"n_cnn_layers\" : 3,\n",
    "        \"n_rnn_layers\" : 3,\n",
    "        \"rnn_dim\"      : [150, 100, 50],\n",
    "        \"n_class\"      : N_CLASSES,\n",
    "        \"out_ch\"       : [8,8,16],\n",
    "        \"dropout\"      : 0.3,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\"   : batch_size,\n",
    "        \"num_workers\"  : num_workers,\n",
    "        \"epochs\"       : epochs\n",
    "        }\n",
    "\n",
    "# define train y test de df_meta\n",
    "test_id  = ['PIT-RNS1090', 'PIT-RNS8973', 'PIT-RNS1438', 'PIT-RNS8326', 'PIT-RNS3016']\n",
    "vali_id  = ['PIT-RNS1603', 'PIT-RNS1556', 'PIT-RNS1534', 'PIT-RNS6989', 'PIT-RNS2543', 'PIT-RNS7168', 'PIT-RNS6762']\n",
    "\n",
    "\n",
    "train_df = df_meta.copy()\n",
    "test_df  = pd.DataFrame()\n",
    "vali_df  = pd.DataFrame()\n",
    "\n",
    "for s in range (len(test_id)):\n",
    "    test_df = pd.concat([test_df, df_meta[df_meta['rns_id'] == test_id[s]]])\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "    train_df.drop(train_df[train_df['rns_id'] == test_id[s]].index, inplace = True)\n",
    "\n",
    "for s in range(len(vali_id)):\n",
    "    vali_df=pd.concat([vali_df, df_meta[df_meta['rns_id'] == vali_id[s]]])\n",
    "    vali_df.reset_index(drop=True, inplace=True)\n",
    "    train_df.drop(train_df[train_df['rns_id'] == vali_id[s]].index, inplace = True)\n",
    "\n",
    "# experimentos que se van a realizar\n",
    "experiments = 'exp3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for: exp3.2\n"
     ]
    }
   ],
   "source": [
    "model = iESPnet(\n",
    "                hparams['n_cnn_layers'],\n",
    "                hparams['n_rnn_layers'],\n",
    "                hparams['rnn_dim'],\n",
    "                hparams['n_class'],\n",
    "                hparams['out_ch'],\n",
    "                hparams['dropout'],\n",
    "                )\n",
    "\n",
    "save_runs        = save_path + experiments +  '/runs/'\n",
    "save_models      = save_path + experiments +  '/models/'\n",
    "save_predictions = save_path + experiments +  '/results/'\n",
    "save_figs        = save_path + experiments +  '/figs/'\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "if not os.path.exists(save_runs):\n",
    "    os.makedirs(save_runs)\n",
    "\n",
    "if not os.path.exists(save_models):\n",
    "    os.makedirs(save_models)\n",
    "\n",
    "if not os.path.exists(save_predictions):\n",
    "    os.makedirs(save_predictions)\n",
    "\n",
    "if not os.path.exists(save_figs):\n",
    "    os.makedirs(save_figs)\n",
    "\n",
    "print('Running training for: ' + experiments)\n",
    "\n",
    "# Dataloaders creados\n",
    "train_data_orig = SeizureDatasetLabelTime(\n",
    "                                          file             = train_df,\n",
    "                                          root_dir         = SPE_DIR,\n",
    "                                          transform        = None, \n",
    "                                          target_transform = smoothing_label(),\n",
    "                                         )\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "                                      T.FrequencyMasking(FREQ_MASK_PARAM),\n",
    "                                      T.TimeMasking(TIME_MASK_PARAN), \n",
    "                                      permute_spec()                                                                     \n",
    "                                    ])\n",
    "\n",
    "# data augmentation only in train data\n",
    "train_data_tran = SeizureDatasetLabelTime(\n",
    "                                          file             = train_df,\n",
    "                                          root_dir         = SPE_DIR,\n",
    "                                          transform        = transform_train, \n",
    "                                          target_transform = smoothing_label() \n",
    "                                         )\n",
    "\n",
    "train_data = torch.utils.data.ConcatDataset([train_data_orig, train_data_tran])\n",
    "\n",
    "# testing data should be balanced, just be \"as it is\"\n",
    "test_data       = SeizureDatasetLabelTime(\n",
    "                                          file             = test_df,\n",
    "                                          root_dir         = SPE_DIR,\n",
    "                                          transform        = None,\n",
    "                                          target_transform = smoothing_label()  \n",
    "                                         )\n",
    "\n",
    "# validation data should be balanced, just be \"as it is\"\n",
    "vali_data       = SeizureDatasetLabelTime(\n",
    "                                          file             = vali_df,\n",
    "                                          root_dir         = SPE_DIR,\n",
    "                                          transform        = None,\n",
    "                                          target_transform = smoothing_label()  \n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# se debe balancear train_df\n",
    "weights = make_weights_for_balanced_classes(train_df, [0,1], n_concat=2)\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(weights) != len(train_data):\n",
    "    AssertionError('sampler should be equal to train data shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# train model until the indicated number of epochs\n",
    "# to track the average training loss per epoch as the model trains\n",
    "avg_train_losses = []\n",
    "train_accs       = []\n",
    "\n",
    "# to track the average validation loss per epoch as the model trains\n",
    "avg_valid_losses = [] \n",
    "valid_accs       = []\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "# following pytorch suggestion to speed up training\n",
    "torch.backends.cudnn.benchmark     = False # reproducibilidad\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "kwargs = {'num_workers': hparams[\"num_workers\"], 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = DataLoader(train_data, batch_size = hparams[\"batch_size\"], sampler = sampler, **kwargs)\n",
    "valid_loader = DataLoader(vali_data, batch_size = hparams[\"batch_size\"], shuffle = False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Model Parameters 1654837\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "#move model to device\n",
    "model.to(device)\n",
    "\n",
    "print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'], weight_decay=1e-4)\n",
    "\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "                                            optimizer, \n",
    "                                            max_lr          = hparams['learning_rate'], \n",
    "                                            steps_per_epoch = int(len(train_loader)),\n",
    "                                            epochs          = hparams['epochs'],\n",
    "                                            anneal_strategy = 'linear'\n",
    "                                            )\n",
    "        \n",
    "criterion = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 4, 120, 181])\n"
     ]
    }
   ],
   "source": [
    "train_loss   = 0.0\n",
    "train_losses = []\n",
    "\n",
    "# precision = Precision(average=False, device=device)\n",
    "# recall    = Recall(average=False, device=device)\n",
    "\n",
    "cont = 0\n",
    "model.train()\n",
    "\n",
    "for batch_idx, _data in enumerate(train_loader):\n",
    "    cont+=1\n",
    "    \n",
    "    spectrograms, labels = _data\n",
    "    spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_thalamus)",
   "language": "python",
   "name": "env_thalamus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
