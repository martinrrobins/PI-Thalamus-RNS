{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pimera experimentación: estudios de ablación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sin normalizar:\n",
    "    1. top_db = 40\n",
    "    2. top_db = 60\n",
    "    3. top_db = 80 \n",
    "\n",
    "\n",
    "2. Normalización por canal:\n",
    "    1. top_db = 40\n",
    "    2. top_db = 60\n",
    "    3. top_db = 80 \n",
    "\n",
    "\n",
    "3. Normalización global:\n",
    "    1. top_db = 40\n",
    "    2. top_db = 60\n",
    "    3. top_db = 80 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thalamus\n",
    "\n",
    "- PIT-RNS0427 -- Train\n",
    "- PIT-RNS1713 -- Train\n",
    "- PIT-RNS8326 -- Test\n",
    "- PIT-RNS3016 -- Test\n",
    "- PIT-RNS7168 -- Val\n",
    "- PIT-RNS6762 -- Val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. grupo de entrenamiento de: 18 pacietes\n",
    "\n",
    "\n",
    "2. grupo de testeo de: 5 pacientes\n",
    "\n",
    "\n",
    "3. grupo de validación: 7 pacientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 estudio de ablación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import torchaudio.transforms    as T\n",
    "import torch.optim              as optim\n",
    "import pandas                   as pd\n",
    "import numpy                    as np\n",
    "\n",
    "from torchvision       import transforms\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..','..','iESPnet_SRC_main','utilities')))\n",
    "from Generator         import SeizureDatasetLabelTimev2, permute_spec, smoothing_label\n",
    "from Model             import iESPnet\n",
    "from TrainEval         import train_model_v2, test_model_v2, get_performance_indices\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..','05-Train-Test')))\n",
    "from utilit_train_test import make_weights_for_balanced_classes\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('../../..','03 Dynamic-Spatial-Filtering')))\n",
    "from models            import DynamicSpatialFilter\n",
    "\n",
    "# set the seed for reproducibility\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direccion donde se encuentran los espectrogramas \n",
    "SPE_DIR        = '/media/martin/Disco2/Rns_Data/PITT_PI_EEG_PROCESS/'\n",
    "meta_data_file = '/media/martin/Disco2/Rns_Data/PITT_PI_EEG_PROCESS/METADATA/allfiles_metadata.csv'\n",
    "\n",
    "df_meta        = pd.read_csv(meta_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables iESPnet\n",
    "FREQ_MASK_PARAM    = 10\n",
    "TIME_MASK_PARAN    = 20\n",
    "N_CLASSES          = 1\n",
    "learning_rate      = 1e-3\n",
    "batch_size         = 64    #128\n",
    "epochs             = 20\n",
    "num_workers        = 4\n",
    "\n",
    "save_path          = 'models_DSF_iESPnet/'\n",
    "patients           = df_meta['rns_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables DSF\n",
    "denoising          = 'autoreject'   # 'autoreject' 'data_augm' \n",
    "model              = 'stager_net'\n",
    "dsf_type           = 'dsfd'         # 'dsfd' 'dsfm_st'\n",
    "mlp_input          = 'log_diag_cov'\n",
    "dsf_soft_thresh    = False\n",
    "dsf_n_out_channels = None\n",
    "n_channels         = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiperparametros iESPnet\n",
    "hparams = {\n",
    "           \"n_cnn_layers\" : 3,\n",
    "           \"n_rnn_layers\" : 3,\n",
    "           \"rnn_dim\"      : [150, 100, 50],\n",
    "           \"n_class\"      : N_CLASSES,\n",
    "           \"out_ch\"       : [8,8,16],\n",
    "           \"dropout\"      : 0.3,\n",
    "           \"learning_rate\": learning_rate,\n",
    "           \"batch_size\"   : batch_size,\n",
    "           \"num_workers\"  : num_workers,\n",
    "           \"epochs\"       : epochs\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train y test de df_meta\n",
    "\n",
    "test_id  = ['PIT-RNS1090', 'PIT-RNS8973', 'PIT-RNS1438', 'PIT-RNS8326', 'PIT-RNS3016']\n",
    "vali_id  = ['PIT-RNS1603', 'PIT-RNS1556', 'PIT-RNS1534', 'PIT-RNS6989', 'PIT-RNS2543', 'PIT-RNS7168', 'PIT-RNS6762']\n",
    "\n",
    "\n",
    "train_df = df_meta.copy()\n",
    "test_df  = pd.DataFrame()\n",
    "vali_df  = pd.DataFrame()\n",
    "\n",
    "for s in range (len(test_id)):\n",
    "    test_df = pd.concat([test_df, df_meta[df_meta['rns_id'] == test_id[s]]])\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "    train_df.drop(train_df[train_df['rns_id'] == test_id[s]].index, inplace = True)\n",
    "\n",
    "for s in range(len(vali_id)):\n",
    "    vali_df=pd.concat([vali_df, df_meta[df_meta['rns_id'] == vali_id[s]]])\n",
    "    vali_df.reset_index(drop=True, inplace=True)\n",
    "    train_df.drop(train_df[train_df['rns_id'] == vali_id[s]].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df['rns_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df['rns_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vali_df['rns_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimentos que se van a realizar\n",
    "experiments_1 = ['exp1','exp2','exp3']\n",
    "experiments_2 = ['.1','.2','.3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exp1.1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = experiments_1[0] + experiments_2[0]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "1 0\n",
      "1 1\n",
      "1 2\n",
      "2 0\n",
      "2 1\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "for s in range (len(experiments_1)):\n",
    "    for j in range (len(experiments_2)):\n",
    "        print(s,j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo s = 0\n",
    "\n",
    "s = 0\n",
    "j = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = DynamicSpatialFilter(\n",
    "                              n_channels, \n",
    "                              mlp_input            = mlp_input, \n",
    "                              n_out_channels       = dsf_n_out_channels, \n",
    "                              apply_soft_thresh    = dsf_soft_thresh\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = iESPnet(\n",
    "                 hparams['n_cnn_layers'],\n",
    "                 hparams['n_rnn_layers'],\n",
    "                 hparams['rnn_dim'],\n",
    "                 hparams['n_class'],\n",
    "                 hparams['out_ch'],\n",
    "                 hparams['dropout'],\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_runs        = save_path + experiments_1[s] + '/' + str(experiments_1[s]) + str(experiments_2[j]) +'/runs/'\n",
    "save_models      = save_path + experiments_1[s] + '/' + str(experiments_1[s]) + str(experiments_2[j]) + '/models/'\n",
    "save_predictions = save_path + experiments_1[s] + '/' + str(experiments_1[s]) + str(experiments_2[j]) + '/results/'\n",
    "save_figs        = save_path + experiments_1[s] + '/' + str(experiments_1[s]) + str(experiments_2[j]) + '/figs/'\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    \n",
    "if not os.path.exists(save_runs):\n",
    "    os.makedirs(save_runs)\n",
    "    \n",
    "if not os.path.exists(save_models):\n",
    "    os.makedirs(save_models)\n",
    "    \n",
    "if not os.path.exists(save_predictions):\n",
    "    os.makedirs(save_predictions)\n",
    "    \n",
    "if not os.path.exists(save_figs):\n",
    "    os.makedirs(save_figs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for: exp1.3\n"
     ]
    }
   ],
   "source": [
    "print('Running training for: ' + experiments_1[s] +  experiments_2[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders creados\n",
    "train_data = SeizureDatasetLabelTimev2(\n",
    "                                       file             = train_df,\n",
    "                                       root_dir         = SPE_DIR,\n",
    "                                       transform        = None, \n",
    "                                       target_transform = smoothing_label(),\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing data should be balanced, just be \"as it is\"\n",
    "test_data = SeizureDatasetLabelTimev2(\n",
    "                                      file             = test_df,\n",
    "                                      root_dir         = SPE_DIR,\n",
    "                                      transform        = None,\n",
    "                                      target_transform = smoothing_label()  \n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data should be balanced, just be \"as it is\"\n",
    "vali_data = SeizureDatasetLabelTimev2(\n",
    "                                      file             = vali_df,\n",
    "                                      root_dir         = SPE_DIR,\n",
    "                                      transform        = None,\n",
    "                                      target_transform = smoothing_label()  \n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation \n",
    "transform_train = transforms.Compose([\n",
    "                                      T.FrequencyMasking(FREQ_MASK_PARAM),\n",
    "                                      T.TimeMasking(TIME_MASK_PARAN), \n",
    "                                      permute_spec()                                                                     \n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = make_weights_for_balanced_classes(train_df, [0,1], n_concat=1)\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models_DSF_iESPnet/exp1/exp1.3/models/model_exp1.3'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputfile = save_models + 'model_' + str(experiments_1[s] + experiments_2[j])\n",
    "outputfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train_losses, train_accs, avg_valid_losses, valid_accs = train_model_v2(\n",
    "                                                                            model1, \n",
    "                                                                            model2, \n",
    "                                                                            hparams, \n",
    "                                                                            epochs, \n",
    "                                                                            train_data, \n",
    "                                                                            vali_data, \n",
    "                                                                            transform_train, \n",
    "                                                                            sampler, \n",
    "                                                                            outputfile,\n",
    "                                                                            experiments_1[s],\n",
    "                                                                            experiments_2[j]\n",
    "                                                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models_DSF_iESPnet/exp1/exp1.1/models/model_exp1.1.pth'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_thr = 0.2\n",
    "best_path = outputfile + '.pth'\n",
    "best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in validation\n",
    "outputs_vali = test_model_v2(model1, model2, hparams, best_path, vali_data, experiments_1[s], experiments_1[j])\n",
    "prediction_va = get_performance_indices(outputs_vali['y_true'], outputs_vali['y_prob'], best_thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in testing\n",
    "outputs_test  = test_model_v2(model1, model2, hparams, best_path, test_data, experiments_1[s], experiments_1[j])\n",
    "prediction_te = get_performance_indices(outputs_test['y_true'], outputs_test['y_prob'], best_thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in training\n",
    "outputs_train = test_model_v2(model1, model2, hparams, best_path, train_data, experiments_1[s], experiments_1[j])\n",
    "prediction_tr = get_performance_indices(outputs_train['y_true'], outputs_train['y_prob'], best_thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ = { \n",
    "            \"train_losses\" : avg_train_losses,\n",
    "            \"train_acupr\"  : train_accs,\n",
    "            \"valid_losses\" : avg_valid_losses, \n",
    "            \"valid_acupr\"  : valid_accs,\n",
    "            \"prediction_va\": prediction_va, \n",
    "            \"prediction_te\": prediction_te,\n",
    "            \"prediction_tr\": prediction_tr, \n",
    "            \"hparams\"      : hparams, \n",
    "            \"threshold\"    : 0.2, \n",
    "            \"train_size\"   : len(train_data)/len(df_meta) # verificar tamaño de train data\n",
    "            }\n",
    "\n",
    "np.save(save_predictions + 'results.npy', predict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models_DSF_iESPnet/exp1/exp1.1/results/results.npy'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = save_predictions + 'results.npy'\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_thalamus)",
   "language": "python",
   "name": "env_thalamus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
