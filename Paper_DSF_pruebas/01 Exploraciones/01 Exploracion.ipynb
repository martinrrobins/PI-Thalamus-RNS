{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "\n",
    "import mne\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from mne.datasets.utils import _get_path\n",
    "from mne.datasets.sleep_physionet._utils import _fetch_one\n",
    "from braindecode.datasets import BaseDataset, BaseConcatDataset\n",
    "from braindecode.preprocessing.preprocess import _preprocess, preprocess, Preprocessor\n",
    "from braindecode.preprocessing.windowers  import _create_windows_from_events\n",
    "from functools import partial\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC18_DIR           = op.join('..','..','..','03 Dynamic-Spatial-Filtering', 'data', 'pc18')\n",
    "PC18_RECORDS       = op.join(PC18_DIR, 'sleep_records.csv')\n",
    "PC18_INFO          = op.join(PC18_DIR, 'age-sex.csv')\n",
    "PC18_SHA1_TRAINING = op.join(PC18_DIR, 'training_SHA1SUMS')\n",
    "PC18_SHA1_TEST     = op.join(PC18_DIR, 'test_SHA1SUMS')\n",
    "PC18_URL           = 'https://physionet.org/files/challenge-2018/1.0.0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_records = pd.read_csv(PC18_RECORDS)\n",
    "df_info    = pd.read_csv(PC18_INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Record</th>\n",
       "      <th>Record type</th>\n",
       "      <th>Split</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>sha</th>\n",
       "      <th>fname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>te03-0024</td>\n",
       "      <td>PSG</td>\n",
       "      <td>test</td>\n",
       "      <td>31</td>\n",
       "      <td>male</td>\n",
       "      <td>fe9a52b00a81a8c2c29ec30b4f8e0d21e0d1d0b8</td>\n",
       "      <td>test/te03-0024/te03-0024.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>te03-0024</td>\n",
       "      <td>Header</td>\n",
       "      <td>test</td>\n",
       "      <td>31</td>\n",
       "      <td>male</td>\n",
       "      <td>52b31029da8454f01610745c27eb07d0bda7c301</td>\n",
       "      <td>test/te03-0024/te03-0024.hea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>te03-0031</td>\n",
       "      <td>Header</td>\n",
       "      <td>test</td>\n",
       "      <td>55</td>\n",
       "      <td>male</td>\n",
       "      <td>573dc478c0219adb9c66229222dcd3b3a5febafb</td>\n",
       "      <td>test/te03-0031/te03-0031.hea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>te03-0031</td>\n",
       "      <td>PSG</td>\n",
       "      <td>test</td>\n",
       "      <td>55</td>\n",
       "      <td>male</td>\n",
       "      <td>de56e1df7cb0d8856eeb900f182e9a3b7f96a4f7</td>\n",
       "      <td>test/te03-0031/te03-0031.mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>te03-0032</td>\n",
       "      <td>Header</td>\n",
       "      <td>test</td>\n",
       "      <td>50</td>\n",
       "      <td>male</td>\n",
       "      <td>1fe5c7bd48e9346da4e1082cea42483a2b3f5469</td>\n",
       "      <td>test/te03-0032/te03-0032.hea</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject     Record Record type Split  Age   Sex  \\\n",
       "0        0  te03-0024         PSG  test   31  male   \n",
       "1        0  te03-0024      Header  test   31  male   \n",
       "2        1  te03-0031      Header  test   55  male   \n",
       "3        1  te03-0031         PSG  test   55  male   \n",
       "4        2  te03-0032      Header  test   50  male   \n",
       "\n",
       "                                        sha                         fname  \n",
       "0  fe9a52b00a81a8c2c29ec30b4f8e0d21e0d1d0b8  test/te03-0024/te03-0024.mat  \n",
       "1  52b31029da8454f01610745c27eb07d0bda7c301  test/te03-0024/te03-0024.hea  \n",
       "2  573dc478c0219adb9c66229222dcd3b3a5febafb  test/te03-0031/te03-0031.hea  \n",
       "3  de56e1df7cb0d8856eeb900f182e9a3b7f96a4f7  test/te03-0031/te03-0031.mat  \n",
       "4  1fe5c7bd48e9346da4e1082cea42483a2b3f5469  test/te03-0032/te03-0032.hea  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4960 entries, 0 to 4959\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Subject      4960 non-null   int64 \n",
      " 1   Record       4960 non-null   object\n",
      " 2   Record type  4960 non-null   object\n",
      " 3   Split        4960 non-null   object\n",
      " 4   Age          4960 non-null   int64 \n",
      " 5   Sex          4960 non-null   object\n",
      " 6   sha          4960 non-null   object\n",
      " 7   fname        4960 non-null   object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 310.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_records.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PSG', 'Header', 'Arousal'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_records['Record type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Record</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>tr14-0268</td>\n",
       "      <td>M</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>tr14-0272</td>\n",
       "      <td>F</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>tr14-0276</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>tr14-0278</td>\n",
       "      <td>F</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>tr14-0291</td>\n",
       "      <td>M</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Record Sex  Age\n",
       "1978  tr14-0268   M   49\n",
       "1979  tr14-0272   F   62\n",
       "1980  tr14-0276   M   32\n",
       "1981  tr14-0278   F   73\n",
       "1982  tr14-0291   M   80"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1983 entries, 0 to 1982\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Record  1983 non-null   object\n",
      " 1   Sex     1983 non-null   object\n",
      " 2   Age     1983 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 46.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_info.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tiene una clase que se encuentra en dataset.py que se piensa que es para cargar el set de datos: necesita de otras funciones como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para guardar localmente el dataset\n",
    "\n",
    "def _data_path(path=None, force_update=False, update_path=None, verbose=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Get path to local copy of PC18 dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    key =  'PC18_DATASET_PATH'\n",
    "    name = 'PC18_DATASET_SLEEP'\n",
    "    path = _get_path(path, key, name)\n",
    "    subdirs = os.listdir(path)\n",
    "\n",
    "    if 'training' in subdirs or 'test' in subdirs:  # the specified path is\n",
    "        # already at the training and test folders level\n",
    "        return path\n",
    "    else:\n",
    "        return op.join('/media/martin/Disco2', 'Dsf_Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pc18_data(subjects, path=None, force_update=False, update_path=None, base_url=PC18_URL, verbose=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Get paths to local copies of PhysioNet Challenge 2018 dataset files.\n",
    "\n",
    "    This will fetch data from the publicly available PhysioNet Computing in\n",
    "    Cardiology Challenge 2018 dataset on sleep arousal detection [1]_ [2]_.\n",
    "    This corresponds to 1983 recordings from individual subjects with\n",
    "    (suspected) sleep apnea. The dataset is separated into a training set with\n",
    "    994 recordings for which arousal annotation are available and a test set\n",
    "    with 989 recordings for which the labels have not been revealed. Across the\n",
    "    entire dataset, mean age is 55 years old and 65% of recordings are from\n",
    "    male subjects.\n",
    "\n",
    "    More information can be found on the\n",
    "    `physionet website <https://physionet.org/content/challenge-2018/1.0.0/>`_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subjects : list of int\n",
    "        The subjects to use. Can be in the range of 0-1982 (inclusive). Test\n",
    "        recordings are 0-988, while training recordings are 989-1982.\n",
    "    path : None | str\n",
    "        Location of where to look for the PC18 data storing location. If None,\n",
    "        the environment variable or config parameter ``PC18_DATASET_PATH``\n",
    "        is used. If it doesn't exist, the \"~/mne_data\" directory is used. If\n",
    "        the dataset is not found under the given path, the data will be\n",
    "        automatically downloaded to the specified folder.\n",
    "    force_update : bool\n",
    "        Force update of the dataset even if a local copy exists.\n",
    "    update_path : bool | None\n",
    "        If True, set the PC18_DATASET_PATH in mne-python config to the given\n",
    "        path. If None, the user is prompted.\n",
    "    base_url : str\n",
    "        The URL root.\n",
    "    %(verbose)s\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    paths : list\n",
    "        List of local data paths of the given type.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Mohammad M Ghassemi, Benjamin E Moody, Li-wei H Lehman, Christopher\n",
    "      Song, Qiao Li, Haoqi Sun, Roger G Mark, M Brandon Westover, Gari D\n",
    "      Clifford. You Snooze, You Win: the PhysioNet/Computing in Cardiology\n",
    "      Challenge 2018.\n",
    "    .. [2] Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C.,\n",
    "      Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and\n",
    "      PhysioNet: Components of a new research resource for complex physiologic\n",
    "      signals. Circulation [Online]. 101 (23), pp. e215–e220.)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    records         = pd.read_csv(PC18_RECORDS)\n",
    "    psg_records     = records[records['Record type'] == 'PSG']\n",
    "    hea_records     = records[records['Record type'] == 'Header']\n",
    "    arousal_records = records[records['Record type'] == 'Arousal']\n",
    "\n",
    "    path            = _data_path(path=path, update_path=update_path)\n",
    "    params          = [path, force_update, base_url]\n",
    "\n",
    "    fnames          = []\n",
    "    for subject in subjects:\n",
    "        for idx in np.where(psg_records['Subject'] == subject)[0]:\n",
    "            psg_fname = _fetch_one(psg_records['fname'].iloc[idx], psg_records['sha'].iloc[idx], *params)\n",
    "            hea_fname = _fetch_one(hea_records['fname'].iloc[idx], hea_records['sha'].iloc[idx], *params)\n",
    "            if psg_records['Split'].iloc[idx] == 'training':\n",
    "                train_idx = np.where(\n",
    "                    arousal_records['Subject'] == subject)[0][0]\n",
    "                arousal_fname = _fetch_one(\n",
    "                    arousal_records['fname'].iloc[train_idx],\n",
    "                    arousal_records['sha'].iloc[train_idx], *params)\n",
    "            else:\n",
    "                arousal_fname = None\n",
    "            fnames.append([psg_fname, hea_fname, arousal_fname])\n",
    "\n",
    "    return fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_wfdb_anns_to_mne_annotations(annots):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Convert wfdb.io.Annotation format to MNE's.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    annots : wfdb.io.Annotation\n",
    "        Annotation object obtained by e.g. loading an annotation file with\n",
    "        wfdb.rdann().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mne.Annotations :\n",
    "        MNE Annotations object.\n",
    "\n",
    "    \"\"\"\n",
    "    ann_chs = set(annots.chan)\n",
    "    onsets = annots.sample / annots.fs\n",
    "    new_onset, new_duration, new_description = list(), list(), list()\n",
    "    for ch in ann_chs:\n",
    "        mask = annots.chan == ch\n",
    "        ch_onsets = onsets[mask]\n",
    "        ch_descs = np.array(annots.aux_note)[mask]\n",
    "\n",
    "        # Events with beginning and end, defined by '(event' and 'event)'\n",
    "        if all([(i.startswith('(') or i.endswith(')')) for i in ch_descs]):\n",
    "            pass\n",
    "        else:  # Sleep stage-like annotations\n",
    "            ch_durations = np.concatenate([np.diff(ch_onsets), [30]])\n",
    "            assert all(ch_durations > 0), 'Negative duration'\n",
    "            new_onset.extend(ch_onsets)\n",
    "            new_duration.extend(ch_durations)\n",
    "            new_description.extend(ch_descs)\n",
    "\n",
    "    mne_annots = mne.Annotations(new_onset, new_duration, new_description, orig_time=None)\n",
    "\n",
    "    return mne_annots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PC18(BaseConcatDataset):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Physionet Challenge 2018 polysomnography dataset.\n",
    "\n",
    "    Sleep dataset from https://physionet.org/content/challenge-2018/1.0.0/.\n",
    "    Contains overnight recordings from 1983 healthy subjects.\n",
    "\n",
    "    See `fetch_pc18_data` for a more complete description.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_ids: list(int) | str | None\n",
    "        (list of) int of subject(s) to be loaded. If None, load all available\n",
    "        subjects. If 'training', load all training recordings. If 'test', load\n",
    "        all test recordings.\n",
    "    path : None | str\n",
    "        Location of where to look for the PC18 data storing location. If None,\n",
    "        the environment variable or config parameter ``MNE_DATASETS_PC18_PATH``\n",
    "        is used. If it doesn't exist, the \"~/mne_data\" directory is used. If\n",
    "        the dataset is not found under the given path, the data will be\n",
    "        automatically downloaded to the specified folder.\n",
    "    load_eeg_only: bool\n",
    "        If True, only load the EEG channels and discard the others (EOG, EMG,\n",
    "        temperature, respiration) to avoid resampling the other signals.\n",
    "    preproc : list(Preprocessor) | None\n",
    "        List of preprocessors to apply to each file individually. This way the\n",
    "        data can e.g., be downsampled (temporally and spatially) to limit the\n",
    "        memory usage of the entire Dataset object. This also enables applying\n",
    "        preprocessing in parallel over the recordings.\n",
    "    windower : callable | None\n",
    "        Function to split the raw data into windows. If provided, windowing is\n",
    "        integrated into the loading process (after preprocessing) such that\n",
    "        memory usage is minized while allowing parallelization.\n",
    "    n_jobs : int\n",
    "        Number of parallel processes.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, subject_ids=None, path=None, load_eeg_only=True, preproc=None, windower=None, n_jobs=1):\n",
    "        paths = fetch_pc18_data(subject_ids, path=path)\n",
    "        self.info_df = pd.read_csv(PC18_INFO)\n",
    "\n",
    "        if n_jobs == 1:\n",
    "            all_base_ds = [self._load_raw(subject_id, p[0], p[2], load_eeg_only=load_eeg_only,\n",
    "                preproc=preproc, windower=windower)\n",
    "                for subject_id, p in zip(subject_ids, paths)]\n",
    "        else:\n",
    "            all_base_ds = Parallel(n_jobs=n_jobs)(delayed(self._load_raw)(\n",
    "                subject_id, p[0], p[2], load_eeg_only=load_eeg_only,\n",
    "                preproc=preproc, windower=windower)\n",
    "                for subject_id, p in zip(subject_ids, paths))\n",
    "        super().__init__(all_base_ds)\n",
    "\n",
    "    def _load_raw(self, subj_nb, raw_fname, arousal_fname, load_eeg_only, preproc, windower):\n",
    "        raw_fname     = raw_fname[0] if isinstance(raw_fname, tuple) else raw_fname\n",
    "        arousal_fname = arousal_fname[0] if isinstance(arousal_fname, tuple) else arousal_fname\n",
    "\n",
    "\n",
    "        channel_types = ['eeg'] * 7\n",
    "        if load_eeg_only:\n",
    "            channels  = list(range(7))\n",
    "        else:\n",
    "            channel_types += ['emg', 'misc', 'misc', 'misc', 'misc', 'ecg']\n",
    "            channels  = None\n",
    "\n",
    "        # Load raw signals and header\n",
    "        record = wfdb.io.rdrecord(op.splitext(raw_fname)[0], channels=channels)\n",
    "        \n",
    "\n",
    "        # Convert to right units for MNE (EEG should be in V)\n",
    "        data = record.p_signal.T\n",
    "        data[np.array(record.units) == 'uV'] /= 1e6\n",
    "        data[np.array(record.units) == 'mV'] /= 1e3\n",
    "        info = mne.create_info(record.sig_name, record.fs, channel_types)\n",
    "        out = mne.io.RawArray(data, info)\n",
    "\n",
    "        # Extract annotations\n",
    "        if arousal_fname is not None:\n",
    "            print('hasta aca corre')\n",
    "            annots = wfdb.rdann(\n",
    "                                op.splitext(raw_fname)[0], 'arousal', sampfrom=0, sampto=None,\n",
    "                                shift_samps=False, return_label_elements=['symbol'],\n",
    "                                summarize_labels=False\n",
    "                               )\n",
    "            mne_annots = convert_wfdb_anns_to_mne_annotations(annots)\n",
    "            out.set_annotations(mne_annots)\n",
    "        record_name = op.splitext(op.basename(raw_fname))[0]\n",
    "        record_info = self.info_df[\n",
    "            self.info_df['Record'] == record_name].iloc[0]\n",
    "        if record_info['Record'].startswith('tr'):\n",
    "            split = 'training'\n",
    "        elif record_info['Record'].startswith('te'):\n",
    "            split = 'test'\n",
    "        else:\n",
    "            split = 'unknown'\n",
    "\n",
    "        desc = pd.Series({\n",
    "            'subject': subj_nb,\n",
    "            'record': record_info['Record'],\n",
    "            'split': split,\n",
    "            'age': record_info['Age'],\n",
    "            'sex': record_info['Sex']\n",
    "        }, name='')\n",
    "        out = BaseDataset(out, desc)\n",
    "\n",
    "        if preproc is not None:\n",
    "            _preprocess(out, None, preproc)\n",
    "\n",
    "        if windower is not None:\n",
    "            out = windower(out)\n",
    "            out.windows.load_data()\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default location ~/mne_data for PC18_DATASET_SLEEP...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=7, n_times=4770000\n",
      "    Range : 0 ... 4769999 =      0.000 ... 23849.995 secs\n",
      "Ready.\n",
      "Error loading annotations: Python integer 256 out of bounds for uint8\n",
      "Creating RawArray with float64 data, n_channels=7, n_times=5147000\n",
      "    Range : 0 ... 5146999 =      0.000 ... 25734.995 secs\n",
      "Ready.\n",
      "Error loading annotations: Python integer 256 out of bounds for uint8\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "Python integer 256 out of bounds for uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/martin/Documentos/environments/env_dsf/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"/home/martin/Documentos/environments/env_dsf/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/home/martin/Documentos/environments/env_dsf/lib/python3.10/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/martin/Documentos/environments/env_dsf/lib/python3.10/site-packages/joblib/parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/tmp/ipykernel_116180/4090118157.py\", line 82, in _load_raw\n  File \"/home/martin/Documentos/environments/env_dsf/lib/python3.10/site-packages/wfdb/io/annotation.py\", line 1953, in rdann\n    (sample, label_store, subtype, chan, num, aux_note) = proc_ann_bytes(\n  File \"/home/martin/Documentos/environments/env_dsf/lib/python3.10/site-packages/wfdb/io/annotation.py\", line 2154, in proc_ann_bytes\n    sample_diff, current_label_store, bpi = proc_core_fields(filebytes, bpi)\n  File \"/home/martin/Documentos/environments/env_dsf/lib/python3.10/site-packages/wfdb/io/annotation.py\", line 2240, in proc_core_fields\n    sample_diff += int(filebytes[bpi, 0] + 256 * (filebytes[bpi, 1] & 3))\nOverflowError: Python integer 256 out of bounds for uint8\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m PC18(subject_ids\u001b[38;5;241m=\u001b[39msubject_ids, preproc\u001b[38;5;241m=\u001b[39mpreproc, windower\u001b[38;5;241m=\u001b[39mwindower, n_jobs\u001b[38;5;241m=\u001b[39mn_jobs)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[0;32m---> 37\u001b[0m windows_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpc18_debug\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 34\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(dataset, window_size_s, n_jobs)\u001b[0m\n\u001b[1;32m     25\u001b[0m mapping             \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN3\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m}\n\u001b[1;32m     26\u001b[0m windower            \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m     27\u001b[0m                              _create_windows_from_events, infer_mapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     28\u001b[0m                              infer_window_size_stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, trial_start_offset_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m                              window_stride_samples\u001b[38;5;241m=\u001b[39mwindow_size_samples, mapping\u001b[38;5;241m=\u001b[39mmapping\n\u001b[1;32m     32\u001b[0m                              )\n\u001b[0;32m---> 34\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPC18\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubject_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreproc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreproc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "Cell \u001b[0;32mIn[38], line 50\u001b[0m, in \u001b[0;36mPC18.__init__\u001b[0;34m(self, subject_ids, path, load_eeg_only, preproc, windower, n_jobs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     all_base_ds \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_raw(subject_id, p[\u001b[38;5;241m0\u001b[39m], p[\u001b[38;5;241m2\u001b[39m], load_eeg_only\u001b[38;5;241m=\u001b[39mload_eeg_only,\n\u001b[1;32m     47\u001b[0m         preproc\u001b[38;5;241m=\u001b[39mpreproc, windower\u001b[38;5;241m=\u001b[39mwindower)\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m subject_id, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(subject_ids, paths)]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     all_base_ds \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_raw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubject_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_eeg_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_eeg_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreproc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreproc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindower\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubject_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubject_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(all_base_ds)\n",
      "File \u001b[0;32m~/Documentos/environments/env_dsf/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/environments/env_dsf/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documentos/environments/env_dsf/lib/python3.10/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documentos/environments/env_dsf/lib/python3.10/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/environments/env_dsf/lib/python3.10/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documentos/environments/env_dsf/lib/python3.10/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mOverflowError\u001b[0m: Python integer 256 out of bounds for uint8"
     ]
    }
   ],
   "source": [
    "def scale(x, k):\n",
    "    return k * x\n",
    "\n",
    "def cast(x, dtype):\n",
    "    return x.astype(dtype)\n",
    "\n",
    "def load_data(dataset, window_size_s, n_jobs):\n",
    "    \"\"\"\n",
    "    \n",
    "    Load, preprocess and window data.\n",
    "    \n",
    "    \"\"\"       \n",
    "\n",
    "    subject_ids = [989, 990, 991]\n",
    "    ch_names    = ['F3-M2', 'F4-M1', 'O1-M2', 'O2-M1']\n",
    "    preproc     = [\n",
    "                  Preprocessor('pick_channels', ch_names=ch_names, ordered=True),\n",
    "                  Preprocessor('filter', l_freq=None, h_freq=30, n_jobs=1),\n",
    "                  Preprocessor('resample', sfreq=100., n_jobs=1),\n",
    "                  Preprocessor(scale, k=1e6),\n",
    "                  Preprocessor(cast, dtype=np.float32)\n",
    "                  ]\n",
    "\n",
    "    window_size_samples = int(window_size_s * 100)\n",
    "    mapping             = {'W': 0, 'N1': 1, 'N2': 2, 'N3': 3, 'R': 4}\n",
    "    windower            = partial(\n",
    "                                 _create_windows_from_events, infer_mapping=False,\n",
    "                                 infer_window_size_stride=False, trial_start_offset_samples=0,\n",
    "                                 trial_stop_offset_samples=0,\n",
    "                                 window_size_samples=window_size_samples,\n",
    "                                 window_stride_samples=window_size_samples, mapping=mapping\n",
    "                                 )\n",
    "\n",
    "    dataset = PC18(subject_ids=subject_ids, preproc=preproc, windower=windower, n_jobs=n_jobs)\n",
    "    return dataset\n",
    "\n",
    "windows_dataset = load_data('pc18_debug', 30, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estoy teniendo un problema con la parte de _load_raw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dsf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
